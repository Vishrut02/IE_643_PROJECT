{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2321785,"sourceType":"datasetVersion","datasetId":1401418}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport requests\nimport tarfile\nimport json\nimport numpy as np\nfrom os import path\nfrom PIL import Image\nfrom PIL import ImageFont, ImageDraw\nfrom glob import glob\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom os import path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-18T18:58:13.743841Z","iopub.execute_input":"2024-10-18T18:58:13.744264Z","iopub.status.idle":"2024-10-18T18:58:13.752851Z","shell.execute_reply.started":"2024-10-18T18:58:13.744221Z","shell.execute_reply":"2024-10-18T18:58:13.751425Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import requests\nimport tarfile\n\n# Download the dataset file\nfname = 'examples.tar.gz'\nurl = 'https://dax-cdn.cdn.appdomain.cloud/dax-publaynet/1.0.0/' + fname\nr = requests.get(url)\nopen(fname, 'wb').write(r.content)\n\n# Extract the dataset\ntar = tarfile.open(fname)\ntar.extractall()\ntar.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:58:13.755086Z","iopub.execute_input":"2024-10-18T18:58:13.755527Z","iopub.status.idle":"2024-10-18T18:58:14.018845Z","shell.execute_reply.started":"2024-10-18T18:58:13.755485Z","shell.execute_reply":"2024-10-18T18:58:14.017653Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_path = \"examples/\"\npath.exists(data_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:58:14.020304Z","iopub.execute_input":"2024-10-18T18:58:14.020723Z","iopub.status.idle":"2024-10-18T18:58:14.028291Z","shell.execute_reply.started":"2024-10-18T18:58:14.020681Z","shell.execute_reply":"2024-10-18T18:58:14.027001Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Define color code\ncolors = {'title': (255, 0, 0),\n          'text': (0, 255, 0),\n          'figure': (0, 0, 255),\n          'table': (255, 255, 0),\n          'list': (0, 255, 255)}","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:58:14.030097Z","iopub.execute_input":"2024-10-18T18:58:14.030504Z","iopub.status.idle":"2024-10-18T18:58:14.040519Z","shell.execute_reply.started":"2024-10-18T18:58:14.030464Z","shell.execute_reply":"2024-10-18T18:58:14.039481Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageDraw, ImageFont\n\ndef markup(image, annotations, categories):\n    draw = ImageDraw.Draw(image)\n    font = ImageFont.truetype(\"examples/DejaVuSans.ttf\", 15)\n\n    for annotation in annotations:\n        # Draw bounding box\n        draw.rectangle(\n            (annotation['bbox'][0],\n             annotation['bbox'][1],\n             annotation['bbox'][0] + annotation['bbox'][2],\n             annotation['bbox'][1] + annotation['bbox'][3]),\n            outline='red',\n            width=2\n        )\n\n        # Draw label\n        text = categories[annotation['category_id'] - 1]['name']\n        draw.text((annotation['bbox'][0], annotation['bbox'][1]), text, font=font, fill=(255, 255, 255, 255))\n\n    return image\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:58:14.044073Z","iopub.execute_input":"2024-10-18T18:58:14.044603Z","iopub.status.idle":"2024-10-18T18:58:14.055195Z","shell.execute_reply.started":"2024-10-18T18:58:14.044536Z","shell.execute_reply":"2024-10-18T18:58:14.054028Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport shutil\n\n# Load the annotations JSON file\nannotations_path = '/kaggle/input/documnet-layout-recognition-dataset-publaynet-t0/labels/publaynet/train.json'\nwith open(annotations_path, 'r') as f:\n    annotations = json.load(f)\n\n# Extract images and their annotations\nimage_data = {}\nfor image_info in annotations['images']:\n    image_id = image_info['id']\n    file_name = image_info['file_name']\n    image_data[image_id] = {\n        'file_name': file_name,\n        'annotations': []\n    }\n\nfor ann in annotations['annotations']:\n    image_id = ann['image_id']\n    image_data[image_id]['annotations'].append(ann)\n\n# Now filter annotations for only the images that are in the dataset\nimage_dir = '/kaggle/input/documnet-layout-recognition-dataset-publaynet-t0/train-0/publaynet/train'\nimage_files = {f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))}\n\nfiltered_annotations = {image_id: data for image_id, data in image_data.items() if data['file_name'] in image_files}\n\n# Take the first 2000 images\nfiltered_annotations_2000 = dict(list(filtered_annotations.items())[:2000])\n\n# Create new folder for saving the images and annotations\nnew_image_dir = '/kaggle/working/train_2000_images'\nos.makedirs(new_image_dir, exist_ok=True)\n\n# Copy the images to the new folder\nfor image_id, data in filtered_annotations_2000.items():\n    src_path = os.path.join(image_dir, data['file_name'])\n    dst_path = os.path.join(new_image_dir, data['file_name'])\n    shutil.copyfile(src_path, dst_path)\n\n# Create a new annotations file in COCO format for the first 2000 images\nnew_annotations = {\n    \"images\": [],\n    \"annotations\": [],\n    \"categories\": annotations[\"categories\"]  # Assuming categories stay the same\n}\n\n# Add the relevant images and annotations to the new annotations file\nfor image_id, data in filtered_annotations_2000.items():\n    # Add image info\n    for img_info in annotations['images']:\n        if img_info['id'] == image_id:\n            new_annotations[\"images\"].append(img_info)\n    \n    # Add the annotations for this image\n    for ann in data['annotations']:\n        new_annotations[\"annotations\"].append(ann)\n\n# Save the new annotations file\nnew_annotations_path = '/kaggle/working/train_2000_annotations.json'\nwith open(new_annotations_path, 'w') as f:\n    json.dump(new_annotations, f)\n\nprint(f\"Done! Copied 2000 images and saved the annotations to {new_image_dir} and {new_annotations_path}.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T18:58:14.057033Z","iopub.execute_input":"2024-10-18T18:58:14.057487Z","iopub.status.idle":"2024-10-18T19:01:36.869519Z","shell.execute_reply.started":"2024-10-18T18:58:14.057441Z","shell.execute_reply":"2024-10-18T19:01:36.867530Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Done! Copied 2000 images and saved the annotations to /kaggle/working/train_2000_images and /kaggle/working/train_2000_annotations.json.\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define the folder path to be zipped\nfolder_to_zip = '/kaggle/working/train_2000_images'\n\n# Define the output zip file path (without the .zip extension)\noutput_zip = '/kaggle/working/train_2000_images'\n\n# Create a zip file\nshutil.make_archive(output_zip, 'zip', folder_to_zip)\n\nprint(f\"Zipped folder saved as {output_zip}.zip\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T19:01:36.872697Z","iopub.execute_input":"2024-10-18T19:01:36.873388Z","iopub.status.idle":"2024-10-18T19:02:14.229534Z","shell.execute_reply.started":"2024-10-18T19:01:36.873303Z","shell.execute_reply":"2024-10-18T19:02:14.228140Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Zipped folder saved as /kaggle/working/train_2000_images.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport zipfile\n\n# Define the path to the zip file and the directory to extract it\nfile_path = '/kaggle/input/documnet-layout-recognition-dataset-publaynet-t0/train-0/publaynet/train'\n# Now count the number of images in the extracted directory\nimage_extensions = ['.jpg', '.jpeg', '.png']  # Add more extensions if needed\n\nimage_count = 0\n\nfor root, dirs, files in os.walk(file_path):\n    for file in files:\n        if any(file.lower().endswith(ext) for ext in image_extensions):\n            image_count += 1\n\nprint(f\"Total number of images in train-0: {image_count}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T19:02:14.231333Z","iopub.execute_input":"2024-10-18T19:02:14.231890Z","iopub.status.idle":"2024-10-18T19:03:23.413624Z","shell.execute_reply.started":"2024-10-18T19:02:14.231836Z","shell.execute_reply":"2024-10-18T19:03:23.412289Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Total number of images in train-0: 47958\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\nclass PublayNetDataset(Dataset):\n    def __init__(self, image_dir, annotations, transform=None):\n        self.image_dir = image_dir\n        self.annotations = annotations\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)  # Use the length of the filtered_annotations dictionary\n\n    def __getitem__(self, idx):\n        image_id = list(self.annotations.keys())[idx]\n        img_path = os.path.join(self.image_dir, self.annotations[image_id]['file_name'])\n\n        if not os.path.exists(img_path):\n            raise FileNotFoundError(f\"Image not found: {img_path}\")\n\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        # Get bounding boxes and labels\n        boxes = []\n        labels = []\n        for ann in self.annotations[image_id]['annotations']:\n            xmin, ymin, width, height = ann['bbox']\n            if width > 0 and height > 0:\n                xmax = xmin + width\n                ymax = ymin + height\n                boxes.append([xmin, ymin, xmax, ymax])\n                labels.append(ann['category_id'])\n\n        # Handle cases where there are no valid boxes\n        if len(boxes) == 0:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros((0,), dtype=torch.int64)\n        else:\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {'boxes': boxes, 'labels': labels}\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, target\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T19:03:23.414964Z","iopub.execute_input":"2024-10-18T19:03:23.415344Z","iopub.status.idle":"2024-10-18T19:03:26.648491Z","shell.execute_reply.started":"2024-10-18T19:03:23.415305Z","shell.execute_reply":"2024-10-18T19:03:26.647260Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torchvision\nfrom torchvision.transforms import ToTensor\nimport torch.optim as optim\n\n# Load the pre-trained Faster R-CNN model\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Adjust the number of classes (background + your categories)\nnum_classes = 6  # Define the number of classes you have (including background)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Move the model to the appropriate device (GPU/CPU)\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\n# Prepare the dataset and dataloader\ntrain_dataset = PublayNetDataset(image_dir=image_dir, annotations=filtered_annotations, transform=ToTensor())\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n\n# Define optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    for images, targets in train_loader:\n        images = [image.to(device) for image in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Forward pass\n        loss_dict = model(images, targets)\n\n        # Total loss\n        losses = sum(loss for loss in loss_dict.values())\n        epoch_loss += losses.item()\n\n        # Backpropagation\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T19:03:26.649892Z","iopub.execute_input":"2024-10-18T19:03:26.650767Z","iopub.status.idle":"2024-10-18T19:05:14.275871Z","shell.execute_reply.started":"2024-10-18T19:03:26.650721Z","shell.execute_reply":"2024-10-18T19:05:14.274304Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:01<00:00, 161MB/s]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}